---
title: "TCD Project note III"
author: "Suprasanna Pradhan"
date: "15 October 2019"
output:
  html_document: default
  word_document: default
---

```{r}

#Loading required packages
library(tidyverse)
library(ggplot2)
library(caret)
library(caretEnsemble)
library(psych)
library(Amelia)
library(mice)
library(GGally)
library(gutenbergr)
library(tidytext)
library(dplyr)
library(janeaustenr)
library(stringi)
library(tidyr)
library(rpart)
library(randomForest)

```




##Importing Data set
```{r}
library(readxl)
setwd("C:/Users/SuprasannaPradhan/Documents/My Files/Great Lakes Projects/Capstone Project TCD")
train_data1=read.csv("train_new.csv")
test_data1=read.csv("test_data.csv")
```

```{r}
train_bank <- subset (train_data1, select= -c(1))
test_bank <- subset (test_data1, select= -c(1))
str(train_bank)
str(test_bank)
```
```{r}
table(train_bank$DEFAULT)

```
Scale the data
```{r}
#scale(train_bank)
```





#Simple logit model on all variables
```{r}
set.seed(1080) 
bank_lg<- glm(DEFAULT ~ ., train_bank, family = "binomial"(link="logit"))
summary(bank_lg) 
```
# Revised Logistic Regression Model
```{r}
#View(test_bank)
logit_f1 <- subset(test_bank,select= -c(3,4,7,12,13))
#logit_f1
bank_f1_lg<- glm(DEFAULT ~ ., logit_f1, family = "binomial"(link="logit"))
summary(bank_f1_lg) 
```


```{r}
library(car)
vif(bank_f1_lg)
```
#Predcting
```{r}
logit_pred_f1 = predict.glm(bank_f1_lg,newdata =test_bank,type = "response")
test_bank1<- cbind(test_bank,logit_pred_f1)
```

#Checking Accuracy
```{r}
#View(test_bank1)
table(test_bank$DEFAULT,logit_pred_f1>0.5)
Accuracy = (6672+646)/(6672+340+1342)
Accuracy
```

#Validation on test data
```{r}
library(ROCR)
DTpredROC1 = ROCR::prediction(logit_pred_f1, test_bank$DEFAULT)
perf1 = performance(DTpredROC1, "tpr", "fpr")
plot(perf1)

```




```{r}
auc_lg <- as.numeric(performance(DTpredROC1, "auc")@y.values)
auc_lg 
```






```{r}
KS <- max(attr(perf1, 'y.values')[[1]]-attr(perf1, 'x.values')[[1]])
KS
```

Gini
```{r}
## Gini Coefficient
library(ineq)
gini = ineq(test_bank$DEFAULT, type="Gini")
gini

```


```{r}
###############################################
#CART Model
library(rpart)
library(rpart.plot)
r.ctrl = rpart.control(minsplit = 100, minbucket = 10, cp = 0, xval = 10)
CT_model = rpart(DEFAULT ~ ., data = train_bank, method = "class", control = r.ctrl)
CT_model
rpart.plot(CT_model)

```


```{r}
attributes(CT_model)
CT_model$cptable

```
#Pruning the tree 
```{r}
ptree = prune(CT_model, .0031, "CP")
ptree
rpart.plot(ptree)
CT_model$variable.importance
```
##CART validation on test data
```{r}
predTrain = predict(ptree, newdata = train_bank)
pred_class = predict(ptree, newdata = train_bank[,-6], type = "class")
predDT = predict(ptree, newdata = test_bank)

```

#Validation on test data
```{r}
library(ROCR)
DTpredROC_CT = prediction(predDT[,2],test_bank$DEFAULT)
perf2 =performance(DTpredROC_CT, "tpr", "fpr")
plot(perf2)
Auc <- as.numeric(performance(DTpredROC_CT, "auc")@y.values)
Auc
```
KS
```{r}
KS1 <- max(attr(perf2, 'y.values')[[1]]-attr(perf2, 'x.values')[[1]])
KS1
```

Gini
```{r}
## Gini Coefficient
library(ineq)
gini1 = ineq(test_bank$DEFAULT, type="Gini")
gini1

```
```{r}
#Check classification error using confusion matrix
#table(test_bank$DEFAULT,pred_class )

accuarcy <- (5715+1184)/(5715+1184+1297+804)
accuarcy 

```





#Random Forest 
```{r}
train_bank$DEFAULT=as.factor(train_bank$DEFAULT)
library(randomForest) 
seed=101
set.seed(seed)
RF_model = randomForest(DEFAULT ~ ., data = train_bank, mtry = 3, nodesize =10, ntree =501, importance = TRUE)
print(RF_model)
```


#ploting RF moden
```{r}                                                                                    
plot(RF_model, main="")        
legend("topright", c("OOB", "0", "1"), text.col=1:6, lty=1:3, col=1:3)
title(main="Error Rates Random Forest train_data")
```

#Checking OOB
```{r} 
rf_err_rate <-  as.data.frame(RF_model$err.rate)
rf_err_rate$ID <- seq.int(nrow(rf_err_rate)) 
rf_err_rate[which(rf_err_rate$OOB==min(rf_err_rate$OOB)),] 
min_tree<-min(rf_err_rate[which(rf_err_rate$OOB==min(rf_err_rate$OOB)),]$ID) 
```



```{r}
## List the importance of the variables.
impVar <- round(randomForest::importance(RF_model), 2)
impVar[order(impVar[,3],decreasing = TRUE),]
```
#Variable Importance: Graphical representation 
```{r}
varImpPlot(RF_model) 
```

#Optimal mtry value 

```{r}
#View(train_bank)
tune_rf_model <- tuneRF(x =train_bank[,-c(2,6)],
              y=as.factor(train_bank$DEFAULT),
              mtryStart = 3, 
              ntreeTry=100, 
              stepFactor = 1.5, 
              improve =0.0001, 
              trace=TRUE, 
              plot = TRUE,
              doBest = TRUE,
              nodesize = 10, 
              importance=TRUE
)



```
#Validate RF model on test data
```{r}
pred_RF =predict(tune_rf_model, newdata = test_bank[,-6], type="prob")
pred_RF_CL=predict(tune_rf_model, newdata = test_bank[,-6], type="class")
head(pred_RF)
class(pred_RF)

```

#Deciling andrank order table



```{r}
# deciling
decile <- function(x){
  deciles <- vector(length=10)
  for (i in seq(0.1,1,.1)){
    deciles[i*10] <- quantile(x, i, na.rm=T)
  }
  return (
    ifelse(x<deciles[1], 1,
           ifelse(x<deciles[2], 2,
                  ifelse(x<deciles[3], 3,
                         ifelse(x<deciles[4], 4,
                                ifelse(x<deciles[5], 5,
                                       ifelse(x<deciles[6], 6,
                                              ifelse(x<deciles[7], 7,
                                                     ifelse(x<deciles[8], 8,
                                                            ifelse(x<deciles[9], 9, 10
                                                            ))))))))))
}


test_bank$deciles <- decile(pred_RF[,2])


```
#Rank order
```{r}
library(tidyverse)
library(magrittr)
library(data.table)
library(scales)
tmp_DT = data.table(test_bank)
rank <- tmp_DT[, list(
  cnt = length(DEFAULT), 
  cnt_resp = sum(DEFAULT), 
  cnt_non_resp = sum(DEFAULT ==0)) , 
  by=deciles][order(-deciles)]
rank$rrate <- round (rank$cnt_resp / rank$cnt,4);
rank$cum_resp <- cumsum(rank$cnt_resp)
rank$cum_non_resp <- cumsum(rank$cnt_non_resp)
rank$cum_rel_resp <- round(rank$cum_resp / sum(rank$cnt_resp),4);
rank$cum_rel_non_resp <- round(rank$cum_non_resp /sum(rank$cnt_non_resp),4);
rank$ks <- abs(rank$cum_rel_resp - rank$cum_rel_non_resp);

library(scales)
rank$rrate <- percent(rank$rrate)
rank$cum_rel_resp <- percent(rank$cum_rel_resp)
rank$cum_rel_non_resp <- percent(rank$cum_rel_non_resp)

View(rank)
```
#Validation with test data

```{r}
library(ROCR)
RF_pred = ROCR::prediction(pred_RF[,2], test_bank$DEFAULT)
perfx = performance(RF_pred, "tpr", "fpr")
plot(perfx)
plot(perfx,col="red", main="parameters_ROC")
abline(0,1, lty = 8, col = "grey")

```

#Performance Measures 

### Area Under Curve
```{r}
Auc <- as.numeric(performance(RF_pred, "auc")@y.values)
Auc
```


```{r}
KS <- max(attr(perfx, 'y.values')[[1]]-attr(perfx, 'x.values')[[1]])
KS


```

```{r}
## Gini Coefficient
library(ineq)
gini = ineq(pred_RF[,2], type="Gini")
gini

## Classification Error
with(test_bank,table(DEFAULT,pred_RF_CL))
accuracy <-(5509+1273)/(5509+1273+1503+715)
accuracy
```

#Machine learning approach with Ensemble Methods
```{r}
#knn compare
library(class)
#View(train_bank)
#View(test_bank)
knn_fit<- knn(train = train_bank[,-6], test = test_bank[,-c(6,15)], cl=train_bank$DEFAULT,k =5,prob=TRUE)
knn_chk= table(test_bank$DEFAULT,knn_fit)
knn_chk
accuracy.knn = sum(diag(knn_chk))/sum(knn_chk)
accuracy.knn

```





#Naive Bayes
```{r}
#naive bayes
str(train_bank)
library(e1071)
library(caret)
train_bank$DEFAULT = as.factor(train_bank$DEFAULT)
test_bank$DEFAULT = as.factor(test_bank$DEFAULT)
NB = naiveBayes(x =train_bank[-6], y =train_bank$DEFAULT) 
pred.NB = predict(NB, newdata =test_bank[-6])
pred.NB
tab.NB =table(test_bank[,6], pred.NB)
tab.NB
accuracy.NB = sum(diag(tab.NB))/sum(tab.NB)
accuracy.NB


confusionMatrix(pred.NB, test_bank$DEFAULT)
```

#Naive Bayes classifier

```{r}
#names(train_bank)
#train_bank$DEFAULT = as.integer(train_bank$DEFAULT)
#test_bank$DEFAULT = as.integer(test_bank$DEFAULT)
#install.packages(mlr)
library(mlr)
#Create a classification task for learning on bank Dataet and specify default feature
task = makeClassifTask(data = train_bank,target ="DEFAULT")                                                                            
#Initialize the Naive Bayes classifier
selected_model = makeLearner("classif.naiveBayes")
#Train the model
NB_mlr= train(selected_model,task)
#Read the model learned  
NB_mlr$learner.model
#Predict on the dataset without passing the target feature
predictions_mlr = as.data.frame(predict(NB_mlr, newdata = test_bank[,1:13]))
table(test_bank$DEFAULT)
##Confusion matrix to check accuracy
table(predictions_mlr[,1],test_bank$DEFAULT)
    
```
```{r}

predictions_mlr = as.data.frame(predict(NB_mlr, newdata = test_bank[,-6]))

table(predictions_mlr[,1],test_bank$DEFAULT)
```




#Bagging
```{r}
#Bagging#
library(gbm)          
#install.packages('xgboost')
library(xgboost)      
#install.packages('caret')
library(caret)        
library(ipred)
library(rpart)

bank_bagging <- bagging(DEFAULT ~.,data=train_bank,
                         control=rpart.control(maxdepth=5, minsplit=4))

pred_class <- predict(bank_bagging, test_bank)

tab.bg <- table(test_bank$DEFAULT,pred_class)
accuracy.bg = sum(diag(tab.bg))/sum(tab.bg)
accuracy.bg

table(test_bank$DEFAULT,pred_class)
```
#XGBoost 
```{r}
# XGBoost 
#install.packages('xgboost')
library(xgboost)
set.seed(123)
classifier = xgboost(data = as.matrix(train_bank[,-6]), label = train_bank$DEFAULT, nrounds = 10)
#Predicting the Test set results
y_pred <- predict(classifier, newdata = as.matrix(test_bank[-c(6,15)]))
y_pred = (y_pred >= 0.5)

# Making the Confusion Matrix
cm = table(test_bank$DEFAULT, y_pred)
cm
accuracy.bs = sum(diag(cm))/sum(cm)
accuracy.bs

```
# K-Fold Cross Validation
```{r}
library(caret)
folds_bank = createFolds(train_bank$DEFAULT, k =10)
cv = lapply(folds_bank, function(x) {
tr_fold = train_bank[-x, ]
tt_fold = test_bank[x, ]
classifier = xgboost(data = as.matrix(train_bank[-6]), label = train_bank$DEFAULT, nrounds = 10)
y_pred = predict(classifier, newdata = as.matrix(tt_fold[-c(6,15)]))
y_pred = (y_pred >= 0.5)
cmx= table(tt_fold[,6], y_pred)
accuracy = (cmx[1,1] + cmx[2,1])  / (cmx[1,1] + cmx[2,1] + cmx[1,1] + cmx[2,1])
return(accuracy)
})
accuracy = mean(as.numeric(cv))
accuracy
```



